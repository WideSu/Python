#problem description and goal:
##use supervised learning models to identify customers who are likely to churn in the future
##analyze features that affects user retention
--------------------------------------------------------------------------------------------------------------------------------------------------------------------  
#load data:
import pandas as pd
import numpy as np
df=pd.read_csv('bank.data.csv')
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#part1: data information
##what the data looks like:
df.head(5)
  -> 	RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
    0	        1	  15634602	Hargrave	      619	  France	Female	42	  2	    0.00	             1	        1	            1	  101348.88	      1
    1	        2	  15647311	Hill	          608	  Spain	  Female	41	  1	    83807.86	         1	        0	            1	  112542.58	      0
    2	        3	  15619304	Onio	          502	  France	Female	42	  8	    159660.80	         3	        1	            0	  113931.57	      1
    3	        4	  15701354	Boni	          699	  France	Female	39	  1	    0.00	             2	        0	            0	  93826.63	      0
    4	        5	  15737888	Mitchell	      850	  Spain   Female	43	  2	    125510.82	         1	        1	            1	  79084.10	      0

##number of row and column:
print('Number of row: {}'.format(df.shape[0]))
print('Number of column: {}'.format(df.shape[1]))
  ->Number of rows: 10000
    Number of columns: 16

##check basic information:
df.info()
  -><class 'pandas.core.frame.DataFrame'>
    RangeIndex: 10000 entries, 0 to 9999
    Data columns (total 14 columns):
     #   Column           Non-Null Count  Dtype  
    ---  ------           --------------  -----  
     0   RowNumber        10000 non-null  int64  
     1   CustomerId       10000 non-null  int64  
     2   Surname          10000 non-null  object 
     3   CreditScore      10000 non-null  int64  
     4   Geography        10000 non-null  object 
     5   Gender           10000 non-null  object 
     6   Age              10000 non-null  int64  
     7   Tenure           10000 non-null  int64  
     8   Balance          10000 non-null  float64
     9   NumOfProducts    10000 non-null  int64  
     10  HasCrCard        10000 non-null  int64  
     11  IsActiveMember   10000 non-null  int64  
     12  EstimatedSalary  10000 non-null  float64
     13  Exited           10000 non-null  int64  
     dtypes: float64(2), int64(9), object(3)
     memory usage: 1.1+ MB

##check if the values in each column are unique:
df.nunique()
  ->RowNumber          10000
    CustomerId         10000
    Surname             2932
    CreditScore          460
    Geography              3
    Gender                 2
    Age                   70
    Tenure                11
    Balance             6382
    NumOfProducts          4
    HasCrCard              2
    IsActiveMember         2
    EstimatedSalary     9999
    Exited                 2
    dtype: int64
##those numbers mean how many unique number each column has

##check missing value:
df.isnull().sum() 
  ->RowNumber          0
    CustomerId         0
    Surname            0
    CreditScore        0
    Geography          0
    Gender             0
    Age                0
    Tenure             0
    Balance            0
    NumOfProducts      0
    HasCrCard          0
    IsActiveMember     0
    EstimatedSalary    0
    Exited             0
    dtype: int64
##also, we can check the missing value using df.info()

##statistical information:
###for those data, only 'CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary' are the data that we care about statistical data
df[['CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary']].describe()
  ->	    CreditScore	  Age	          Tenure	      NumOfProducts	Balance	      EstimatedSalary
    count	10000.000000	10000.000000	10000.000000	10000.000000	10000.000000	10000.000000
    mean	650.528800	  38.921800	    5.012800	    1.530200	    76485.889288	100090.239881
    std	  96.653299	    10.487806	    2.892174	    0.581654	    62397.405202	57510.492818
    min	  350.000000	  18.000000	    0.000000	    1.000000	    0.000000	    11.580000
    25%	  584.000000	  32.000000	    3.000000	    1.000000	    0.000000	    51002.110000
    50%	  652.000000	  37.000000	    5.000000	    1.000000	    97198.540000	100193.915000
    75%	  718.000000	  44.000000	    7.000000	    2.000000	    127644.240000	149388.247500
    max	  850.000000	  92.000000	    10.000000	    4.000000	    250898.090000	199992.480000

##boxplot for numerical feature:
_,axss = plt.subplots(2,3, figsize=[20,10]) #this means that the following boxplot will generate 6 graphs (in two rows and three columns), and each graph is 20*10 in size
sns.boxplot(x='Exited', y ='CreditScore', data=df, ax=axss[0][0])
sns.boxplot(x='Exited', y ='Age', data=df, ax=axss[0][1])
sns.boxplot(x='Exited', y ='Tenure', data=df, ax=axss[0][2])
sns.boxplot(x='Exited', y ='NumOfProducts', data=df, ax=axss[1][0])
sns.boxplot(x='Exited', y ='Balance', data=df, ax=axss[1][1])
sns.boxplot(x='Exited', y ='EstimatedSalary', data=df, ax=axss[1][2])
  ->image.1

##understand categorical feature:
_,axss = plt.subplots(2,2, figsize=[20,10])
sns.countplot(x='Exited', hue='Geography', data=df, ax=axss[0][0])
sns.countplot(x='Exited', hue='Gender', data=df, ax=axss[0][1])
sns.countplot(x='Exited', hue='HasCrCard', data=df, ax=axss[1][0])
sns.countplot(x='Exited', hue='IsActiveMember', data=df, ax=axss[1][1])
  ->image.2

##feature correlation:
corr_score = df[['CreditScore', 'Age', 'Tenure', 'NumOfProducts','Balance', 'EstimatedSalary']].corr()
print(corr_score)
  ->                 CreditScore       Age  ...   Balance  EstimatedSalary
    CreditScore         1.000000 -0.003965  ...  0.006268        -0.001384
    Age                -0.003965  1.000000  ...  0.028308        -0.007201
    Tenure              0.000842 -0.009997  ... -0.012254         0.007784
    NumOfProducts       0.012238 -0.030680  ... -0.304180         0.014204
    Balance             0.006268  0.028308  ...  1.000000         0.012797
    EstimatedSalary    -0.001384 -0.007201  ...  0.012797         1.000000
sns.heatmap(corr_score)
  ->image.3
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#part2: feature prepossessing 
##ordinal encoding:
###a classical ordinal feature is gender
df['Gender'] = df['Gender'] == 'Female'
  ->  Gender
	      True	
	      True	
	      True	
	      True	
	      True	
	      False	
	      False	
        True	
	      False	
	      False

##one hot encoding:
df = pd.get_dummies(df, columns=['Geography'], drop_first=False)
df.head(10)
  ->    RowNumber	CustomerId	Surname	CreditScore	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited	Geography_France	Geography_Germany	Geography_Spain
    0	  1	        15634602	  Hargrave	619	      True	  42	  2	    0.00	    1	          1	        1	              101348.88	      1	      1	                0	                0
    1	  2	        15647311	  Hill	    608	      True	  41	  1	    83807.86	1	          0	        1	              112542.58	      0	      0	                0	                1
    2	  3	        15619304	  Onio	    502	      True	  42	  8	    159660.80	3	          1	        0	              113931.57	      1	      1	                0	                0
    3   4	        15701354	  Boni	    699	      True	  39	  1	    0.00	    2	          0	        0	              93826.63	      0	      1	                0	                0
    4	  5	        15737888	  Mitchell	850	      True	  43	  2	    125510.82	1	          1	        1	              79084.10	      0	      0	                0	                1
    5	  6	        15574012	  Chu	      645	      False	  44	  8	    113755.78	2	          1	        0	              149756.71	      1	      0	                0	                1
    6	  7	        15592531	  Bartlett	822	      False	  50	  7	    0.00	    2	          1	        1	              10062.80	      0	      1	                0	                0
    7	  8	        15656148	  Obinna	  376	      True	  29	  4	    115046.74	4	          1	        0	              119346.88	      1	      0	                1	                0
    8	  9	        15792365	  He	      501	      False	  44	  4	    142051.07	2	          0	        1	              74940.50	      0	      1	                0	                0
    9	  10	      15592389	  H?	      684	      False	  27	  2	    134603.88	1	          1	        1	              71725.73	      0	      1	                0	                0

##drop useless column:
column_drop = ['RowNumber','CustomerId','Surname','Exited']
X = df.drop(column_drop, axis=1)
X.head()
  ->CreditScore	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Geography_France	Geography_Germany	Geography_Spain
  0	619	        True	  42	2	      0.00	    1	            1	        1	            101348.88	      1	                0	                0
  1	608	        True	  41	1	      83807.86	1	            0	        1	            112542.58	      0	                0	                1
  2	502	        True	  42	8	      159660.80	3	            1	        0	            113931.57	      1	                0	                0
  3	699	        True	  39	1	      0.00	    2	            0	        0	            93826.63	      1	                0	                0
  4	850	        True	  43	2	      125510.82	1	            1	        1	            79084.10	      0	                0	                1
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#model training and data scaling:
##splite data into training and testing:
from sklearn import model_selection
y = churn_df['Exited'] #target column
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, stratify = y, random_state=1)

##data scaling:
###standardization (x-mean)/std
###normalization (x-x_min)/(x_max-x_min) ->[0,1]
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
##model training and selection:
###build models
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.linear_model import LogisticRegression
###Logistic Regression
classifier_logistic = LogisticRegression()
classifier_logistic.fit(X_train, y_train)
classifier_logistic.predict(X_test) #make prediction
classifier_logistic.score(X_test, y_test) #accuracy of test data
  ->0.808
###K Nearest Neighbors
classifier_KNN = KNeighborsClassifier()
classifier_KNN.fit(X_train, y_train)
classifier_KNN.predict(X_test)
classifier_KNN.score(X_test, y_test)
  ->0.8268  
###Random Forest
classifier_RF = RandomForestClassifier()
classifier_RF.fit(X_train, y_train)
classifier_RF.predict(X_test)
classifier_RF.score(X_test, y_test)
  ->0.8588

##5-fold cross validation:
model_names = ['Logistic Regression','KNN','Random Forest']
model_list = [classifier_logistic, classifier_KNN, classifier_RF]
count = 0
for classifier in model_list:
    cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=5)
    print(cv_score)
    print('Model accuracy of ' + model_names[count] + ' is ' + str(cv_score.mean()))
    count += 1
  ->[0.81933333 0.80666667 0.80666667 0.80933333 0.82      ]
    Model accuracy of Logistic Regression is 0.8124
    [0.82533333 0.836      0.814      0.824      0.832     ]
    Model accuracy of KNN is 0.8262666666666666
    [0.878      0.86066667 0.85266667 0.86333333 0.86133333]
    Model accuracy of Random Forest is 0.8632
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#grid search to find optimal hyperparameters:
##think this method as traversing all possible hyperparameters and find the best one
from sklearn.model_selection import GridSearchCV

##logistic regression case:
###for logistic regression, we care about penalty and C
###penalty are choosen from l1 and l2, C can be choosen from multiple numbers
parameters = {'penalty':('l1', 'l2'),'C':(0.01, 1, 5, 10)}
Grid_LR = GridSearchCV(LogisticRegression(solver='liblinear'),parameters, cv=5) #cv means crossvalidation, 5 means 5-fold
Grid_LR.fit(X_train, y_train)
##the best hyperparameter combination
print_grid_search_metrics(Grid_LR)
  ->Best score: 0.8124
    Best parameters set:
    C:1
    penalty:l2
##best model
best_LR_model = Grid_LR.best_estimator_
print(best_LR_model)
  ->LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,
                   warm_start=False)

##KNN case:
parameters = {'n_neighbors':[1,3,5,7,9]}
Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=5)
Grid_KNN.fit(X_train, y_train)
print_grid_search_metrics(Grid_KNN)
	->Best score: 0.8322666666666667
		Best parameters set:
		n_neighbors:9
best_KNN_model = Grid_KNN.best_estimator_
print(best_KNN_model)
	->KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=None, n_neighbors=9, p=2,
                     weights='uniform')

##RF case:
parameters = {'n_estimators' : [40,60,80]}
Grid_RF = GridSearchCV(RandomForestClassifier(),parameters, cv=5)
Grid_RF.fit(X_train, y_train)
print_grid_search_metrics(Grid_RF)
	->Best score: 0.8633333333333333
		Best parameters set:
		n_estimators:60





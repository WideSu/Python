#goal:
#In this project, we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#basic operation:
##load data:
import numpy as np
import pandas as pd
df = pd.read_csv('data.tsv',sep='\t',header=0,error_bad_lines=Fales) #tsv is tab seperated file (csv is comma seperated file); header=0 means the head is row[0]; error_bad_lines=Fales means automatically deal with possible error.
df.head()
  ->marketplace	customer_id	review_id	      product_id	product_parent	product_title	                                      product_category	star_rating	helpful_votes	total_votes	vine	verified_purchase	review_headline	                            review_body	                                        review_date
  0	US	        3653882	    R3O9SGZBVQBV76	B00FALQ1ZC	937001370	      Invicta Women's 15150 "Angel" 18k Yellow Gold ...	  Watches	          5	          0	            0	          N	    Y	                Five Stars	                                Absolutely love this watch! Get compliments al...	  2015-08-31
  1	US	        14661224	  RKH8BNC3L5DLF	  B00D3RGO20	484010722	      Kenneth Cole New York Women's KC4944 Automatic...	  Watches	          5	          0	            0	          N	    Y	                I love thiswatch it keeps time wonderfully	I love this watch it keeps time wonderfully.	      2015-08-31
  2	US	        27324930	  R2HLE8WKZSU3NL	B00DKYC7TK	361166390	      Ritche 22mm Black Stainless Steel Bracelet Wat...	  Watches	          2	          1	            1	          N	    Y	                Two Stars	                                  Scratches	                                          2015-08-31
  3	US	        7211452	    R31U3UH5AZ42LL	B000EQS1JW	958035625	      Citizen Men's BM8180-03E Eco-Drive Stainless S...	  Watches	          5	          0	            0	          N	    Y	                Five Stars	                                It works well on me. However, I found cheaper ...	  2015-08-31
  4	US	        12733322	  R2SV659OUJ945Y	B00A6GFD7S	765328221	      Orient ER27009B Men's Symphony Automatic Stain...	  Watches	          4	          0	            0	          N	    Y	                Beautiful face, but cheap sounding links	  Beautiful watch face. The band looks nice all...	  2015-08-31


##missing value:
df.info()
df.isnull().sum()
df.dropna(subset=['review_body'],inplace=True) #review_body column is the column that we care about
df.isnull().sum()

##use first 1000 data as training data:
data = df.loc[:1000,'review_body'].tolist() #tolist() transfer matrix to list
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#tokenizing and stemming:
##tokenizing means split sentence into multiple single words
##the key is to find the stop word. for example, xx's this 's indicates the end of word.
import nltk
stopwords = nltk.corpus.stopwords.words('english') #put normal words into stopwords list
stopwords.append("'s")
stopwords.append("'m")
stopwords.append("n't")
stopwords.append("br")

##stemming means put words into the original form. for example, 'learn', 'learns', 'learning' acutually stand for same word 'learn'
##combine tokenizing and stemming:
from nltk.stem.snowball import SnowballStemmer
import re
stemmer = SnowballStemmer('english')
###define function:
def tokenization_and_stemming(text):
  tokens = []
  for word in nltk.word_tokenzize(text): #exclude stopwords and tokenize the document, generate a list of string
    if word.lower() not in stopwords:
      tokens.append(word.lower())
  filtered_tokens = []
  for token in tokens:
    if re.search('[a-zA-Z]',token): #re.search will filter token that does not have letters
      filtered_tokens.append(token)
  stems = [stemmer.stem(t) for t in filtered_tokens]
  return stems

##example:
tokenization_and_stemming(data[0])
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#TF-IDF:
##TF: Term Frequency (wordA in document/total word in document)
##IDF: Inverse Document Frequency (log(total number of documents in corpus)/(number of document where wordA appears + 1))
##use TfidfVectorizer to create tf-idf matrix
tfidf_model = TfidfVectorizer(max_df=0.99,max_features=1000,min_df=0.01,stop_words='english',use_idf=True,tokenizer=tokenization_and_stemming,ngram_range=(1,1))
tfidf_matrix = tfidf_model.fit_transform(data)

##check the parameter 
tfidf_model.get_params()

##get the words
tf_selected_words = tfidf_model.get_feature_names()
print(tf_selected_words)


--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#K-means clustering:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#Latent Dirichlet Allocation:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------

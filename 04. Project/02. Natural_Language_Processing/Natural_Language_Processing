#goal:
#In this project, we use unsupervised learning models to cluster unlabeled documents into different groups, visualize the results and identify their latent topics/structures.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#basic operation:
##load data:
import numpy as np
import pandas as pd
df = pd.read_csv('data.tsv',sep='\t',header=0,error_bad_lines=Fales) #tsv is tab seperated file (csv is comma seperated file); header=0 means the head is row[0]; error_bad_lines=Fales means automatically deal with possible error.
df.head()

##missing value:
df.info()
df.isnull().sum()
df.dropna(subset=['review_body'],inplace=True) #review_body column is the column that we care about
df.isnull().sum()

##use first 1000 data as training data:
data = df.loc[:1000,'review_body'].tolist() #tolist() transfer matrix to list
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#tokenizing and stemming:
##tokenizing means split sentence into multiple single words
##the key is to find the stop word. for example, xx's this 's indicates the end of word.
import nltk
stopwords = nltk.corpus.stopwords.words('english') #put normal words into stopwords list
stopwords.append("'s")
stopwords.append("'m")
stopwords.append("n't")
stopwords.append("br")

##stemming means put words into the original form. for example, 'learn', 'learns', 'learning' acutually stand for same word 'learn'
##combine tokenizing and stemming:
from nltk.stem.snowball import SnowballStemmer
import re
stemmer = SnowballStemmer('english')
###define function:
def tokenization_and_stemming(text):
  tokens = []
  for word in nltk.word_tokenzize(text): #exclude stopwords and tokenize the document, generate a list of string
    if word.lower() not in stopwords:
      tokens.append(word.lower())
  filtered_tokens = []
  for token in tokens:
    if re.search('[a-zA-Z]',token): #re.search will filter token that does not have letters
      filtered_tokens.append(token)
  stems = [stemmer.stem(t) for t in filtered_tokens]
  return stems

##example:
tokenization_and_stemming(data[0])
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#TF-IDF:
##TF: Term Frequency (wordA in document/total word in document)
##IDF: Inverse Document Frequency (log(total number of documents in corpus)/(number of document where wordA appears + 1))
##use TfidfVectorizer to create tf-idf matrix
tfidf_model = TfidfVectorizer(max_df=0.99,max_features=1000,min_df=0.01,stop_words='english',use_idf=True,tokenizer=tokenization_and_stemming,ngram_range=(1,1))
tfidf_matrix = tfidf_model.fit_transform(data)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#K-means clustering:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
#Latent Dirichlet Allocation:
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
